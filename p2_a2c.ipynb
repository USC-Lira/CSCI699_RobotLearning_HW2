{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "kj-EphJ8br_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "CbOOpTfzbvNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some global variables"
      ],
      "metadata": {
        "id": "NdxrBp7sQM_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# maximum number of training episodes\n",
        "NUM_EPISODES = 90\n",
        "# maximum number of steps per episode\n",
        "# CartPole-V0 has a maximum of 200 steps per episodes\n",
        "MAX_EP_STEPS = 200\n",
        "# reward discount factor\n",
        "GAMMA = .6\n",
        "# once MAX_EPISODES or ctrl-c is pressed, number of test episodes to run\n",
        "NUM_TEST_EPISODES = 3\n",
        "# batch size used for the training\n",
        "BATCH_SIZE = 1000\n",
        "# maximum number of transitions stored in the replay buffer\n",
        "MAX_REPLAY_BUFFER_SIZE = BATCH_SIZE * 10\n",
        "# reward that is returned when the episode terminates early (i.e. the controller fails)\n",
        "FAILURE_REWARD = -10.\n",
        "\n",
        "# setting the random seed makes things reproducible\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)"
      ],
      "metadata": {
        "id": "K3dYiD7qQPy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Actor class"
      ],
      "metadata": {
        "id": "Er6HLj_uRT1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    \"\"\"An actor for Actor-Critic reinforcement learning.\n",
        "\n",
        "    This actor represent a stochastic policy. It predicts a distribution over\n",
        "    actions condition on a given state. The distribution can then be sampled to\n",
        "    produce an single control action.\n",
        "\n",
        "    Arguments:\n",
        "        state_dim: an integer, number of states of the system\n",
        "        action_dim: an integer, number of possible actions of the system\n",
        "    Returns:\n",
        "        nothing\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "\n",
        "    # the neural network (input will be state, output is unscaled probability distribution)\n",
        "    # note: the neural network must be entirely linear to support verification\n",
        "    self.nn = nn.Sequential(\n",
        "        nn.Linear(state_dim, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, action_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action, td_error):\n",
        "    \"\"\"Runs the forward pass and gets the expected value\n",
        "\n",
        "    Arguments:\n",
        "        state: a tensor representing a batch of states (batch_size X\n",
        "        state_dim)\n",
        "        action: a tensor of integers representing a batch of actions\n",
        "        (batch_size X 1)\n",
        "        where the integers correspond to the action number (0 indexed)\n",
        "        td_error: a tensor of floats (batch_size X 1) the temporal\n",
        "        differences\n",
        "    Returns:\n",
        "        expected_v: a tensor of the expected reward for each of the\n",
        "        samples in the batch (batch_size X 1)\n",
        "    \"\"\"\n",
        "\n",
        "    action_logits = self.nn(state)\n",
        "    action_probs = nn.Softmax()(action_logits)\n",
        "    clipped_action_probs = torch.clip(action_probs, 1e-14, 1.0)\n",
        "    action_oh = torch.nn.functional.one_hot(action[:, 0].long())\n",
        "    log_action_probs = (torch.log(clipped_action_probs) * action_oh).sum(axis=1, keepdims=True)\n",
        "    expected_v = log_action_probs * td_error\n",
        "    return expected_v\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\"Get an action for a given state by predicting a probability distribution over actions and sampling one.\n",
        "\n",
        "    Arguments:\n",
        "        state: a tensor of size (state_dim) representing the current\n",
        "        state\n",
        "    Returns:\n",
        "        action: an integer (0 indexed) corresponding to the action taken\n",
        "        by the actor\n",
        "    \"\"\"\n",
        "    logits = self.nn(state.unsqueeze(0))\n",
        "    action_probs = nn.Softmax()(logits)\n",
        "    action = np.random.choice(self.action_dim, p=action_probs.detach().numpy()[0, :])\n",
        "    return action"
      ],
      "metadata": {
        "id": "9Wx9DdNqRWqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Critic class"
      ],
      "metadata": {
        "id": "AvWzk8nERXTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim):\n",
        "    \"\"\"A critic for Actor-Critic reinforcement learning.\n",
        "\n",
        "    This critic works by estimating a value function (expected reward-to-go) for\n",
        "    given states. It is trained using Temporal Difference error learning (TD\n",
        "    error).\n",
        "\n",
        "    Arguments:\n",
        "        state_dim: an interger, number of states of the system\n",
        "    Returns:\n",
        "        nothing\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.state_dim = state_dim\n",
        "\n",
        "    ######### Your code starts here #########\n",
        "    # hint: look at the implementation of the actor, the TD error and\n",
        "    # the loss functions described in the writeup.\n",
        "\n",
        "    ######### Your code ends here #########\n",
        "\n",
        "  def forward(self, state, reward, state_next):\n",
        "    \"\"\"Runs the training step\n",
        "\n",
        "    Arguments:\n",
        "        state: a tensor representing a batch of initial states\n",
        "        (batch_size X state_dim)\n",
        "        reward: a tensor representing a batch of rewards (batch_size X\n",
        "        1)\n",
        "        state_next: a tensor representing a batch of 'future states'\n",
        "        (batch_size X state_dim)\n",
        "        each sample (state, reward, state_next) correspond to a given\n",
        "        transition\n",
        "    Returns:\n",
        "        td_error: the td errors of the batch, as a numpy array\n",
        "        (batch_size X 1)\n",
        "    \"\"\"\n",
        "    ######### Your code starts here #########\n",
        "\n",
        "\n",
        "    ######### Your code ends here #########\n",
        "    return td_error"
      ],
      "metadata": {
        "id": "vGhdRH28R7ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference function"
      ],
      "metadata": {
        "id": "SuFIbANjSKdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_actor(env, actor, num_episodes, render=True):\n",
        "  \"\"\"Runs the actor on the environment for num_episodes\n",
        "\n",
        "  Arguments:\n",
        "      env: the openai gym environment\n",
        "      actor: an instance of the Actor class\n",
        "      num_episodes: number of episodes to run the actor for\n",
        "  Returns:\n",
        "      nothing\n",
        "  \"\"\"\n",
        "  for i_episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = torch.Tensor(state)\n",
        "    total_reward = 0.0\n",
        "    for t in range(MAX_EP_STEPS):\n",
        "      if render:\n",
        "        env.render()\n",
        "      action = actor.get_action(state)\n",
        "      state, reward, done, info = env.step(action)\n",
        "      state = torch.Tensor(state)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        print(\"Reward: \", str(total_reward))\n",
        "        break"
      ],
      "metadata": {
        "id": "zQpm0u07SMh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "L2agnpnoSS4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_actor_critic():\n",
        "  # setup the OpenAI gym environment\n",
        "  env = gym.make('CartPole-v0')\n",
        "  env.seed(random_seed)\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  print(\"state dim: \", state_dim)\n",
        "  print(\"action dim: \", action_dim)\n",
        "\n",
        "  # create an actor and a critic network and initialize their variables\n",
        "  actor = Actor(state_dim, action_dim)\n",
        "  critic = Critic(state_dim)\n",
        "\n",
        "  actor_optim = optim.Adam(actor.parameters(), lr=3e-4)\n",
        "  critic_optim = optim.Adam(critic.parameters(), lr=3e-4)\n",
        "\n",
        "  # the replay buffer will store observed transitions\n",
        "  replay_buffer = np.zeros((0, 2 * state_dim + 2))\n",
        "\n",
        "  # allocate memory to keep track of episode rewards\n",
        "  reward_history = np.zeros(NUM_EPISODES)\n",
        "\n",
        "  for i_episode in range(NUM_EPISODES):\n",
        "    # very inneficient way of making sure the buffer isn't too full\n",
        "    if replay_buffer.shape[0] > MAX_REPLAY_BUFFER_SIZE:\n",
        "      replay_buffer = replay_buffer[-MAX_REPLAY_BUFFER_SIZE:, :]\n",
        "\n",
        "    # reset the OpenAI gym environment to a random initial state for each episode\n",
        "    state = env.reset()\n",
        "    state = torch.Tensor(state)\n",
        "    episode_reward = 0.0\n",
        "\n",
        "    for t in range(MAX_EP_STEPS):\n",
        "      # uses the actor to get an action at the current state\n",
        "      action = actor.get_action(state)\n",
        "      # call gym to get the next state and reward, given we are taking action at the current state\n",
        "      state_next, reward, done, info = env.step(action)\n",
        "      state_next = torch.Tensor(state_next)\n",
        "\n",
        "      # done=True means either the cartpole failed OR we've reached the maximum number of episode steps\n",
        "      if done and t < (MAX_EP_STEPS - 1):\n",
        "        reward = FAILURE_REWARD\n",
        "      # accumulate the reward for this whole episode\n",
        "      episode_reward += reward\n",
        "      # store the observed transition in our replay buffer for training\n",
        "      replay_buffer = np.vstack(\n",
        "          (replay_buffer, np.hstack((state, action, reward, state_next)))\n",
        "      )\n",
        "\n",
        "      # if our replay buffer has accumulated enough samples, we start learning the actor and the critic\n",
        "      if replay_buffer.shape[0] >= BATCH_SIZE:\n",
        "        # we sample BATCH_SIZE transition from our replay buffer\n",
        "        samples_i = np.random.choice(\n",
        "            replay_buffer.shape[0], BATCH_SIZE, replace=False\n",
        "        )\n",
        "        state_samples = torch.Tensor(replay_buffer[samples_i, 0:state_dim])\n",
        "        action_samples = torch.Tensor(replay_buffer[samples_i, state_dim : state_dim + 1])\n",
        "        reward_samples = torch.Tensor(replay_buffer[samples_i, state_dim + 1 : state_dim + 2])\n",
        "        state_next_samples = torch.Tensor(replay_buffer[\n",
        "            samples_i, state_dim + 2 : 2 * state_dim + 2\n",
        "        ])\n",
        "\n",
        "        # compute the TD error using the critic\n",
        "        actor_optim.zero_grad()\n",
        "        critic_optim.zero_grad()\n",
        "\n",
        "        td_error = critic(\n",
        "            state_samples, reward_samples, state_next_samples\n",
        "        )\n",
        "        critic_loss = torch.square(td_error).mean()\n",
        "\n",
        "        critic_loss.backward()\n",
        "        critic_optim.step()\n",
        "\n",
        "        # train the actor (we don't need the expected value unless you want to log it)\n",
        "        expected_v = actor(state_samples, action_samples, td_error.detach())\n",
        "        actor_loss = -expected_v.mean()\n",
        "        actor_loss.backward()\n",
        "        actor_optim.step()\n",
        "\n",
        "        if done:\n",
        "          # print how well we did on this episode\n",
        "          print(episode_reward)\n",
        "          reward_history[i_episode] = episode_reward\n",
        "\n",
        "      # update current state for next iteration\n",
        "      state = state_next\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "    reward_history[i_episode] = episode_reward\n",
        "\n",
        "  # plot reward history\n",
        "  plt.figure()\n",
        "  plt.plot(reward_history)\n",
        "  plt.xlabel('Number of Episodes')\n",
        "  plt.ylabel('Episode Reward')\n",
        "  plt.title('History of Episode Reward')\n",
        "  if not os.path.exists('../plots'):\n",
        "    os.makedirs('../plots')\n",
        "  plt.savefig('../plots/p2_reward_history.png')\n",
        "  plt.show()\n",
        "  run_actor(env, actor, NUM_TEST_EPISODES)\n",
        "\n",
        "  # closes the environement\n",
        "  env.close()"
      ],
      "metadata": {
        "id": "jEZM1bOMQH-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run everything together"
      ],
      "metadata": {
        "id": "mw2Q8cVkUb11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_actor_critic()"
      ],
      "metadata": {
        "id": "mO8fk3WeUc5C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}